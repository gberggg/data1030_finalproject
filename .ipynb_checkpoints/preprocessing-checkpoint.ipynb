{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9e383e-04d6-4adf-8bf5-e9251de6a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train: (5256, 6)\n",
      "Shape of y train: (5256, 2)\n",
      "Shape of X val: (1752, 6)\n",
      "Shape of y val: (1752, 2)\n",
      "Shape of X test: (1752, 6)\n",
      "Shape of y test: (1752, 2)\n",
      "   Unnamed: 0  no_wrk_aux  no2_wrk_aux  o3_wrk_aux       temp         rh\n",
      "0         215    0.028002     0.018061    0.032134   3.294843  52.182907\n",
      "1        7818    0.030186     0.015001    0.029275  11.455113  65.229446\n",
      "2        7068    0.028858     0.014326    0.029976  19.665237  35.056055\n",
      "3        5290    0.045463     0.000057    0.017437  33.924103  31.951930\n",
      "4        4198    0.029843     0.016397    0.035596  25.440354  64.711433\n"
     ]
    }
   ],
   "source": [
    "# import split sets\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv(\"split_data/X_train.csv\")\n",
    "X_val = pd.read_csv(\"split_data/X_val.csv\")\n",
    "X_test = pd.read_csv(\"split_data/X_test.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"split_data/y_train.csv\")\n",
    "y_val = pd.read_csv(\"split_data/y_val.csv\")\n",
    "y_test = pd.read_csv(\"split_data/y_test.csv\")\n",
    "\n",
    "# verify shapes\n",
    "print(\"Shape of X train:\", X_train.shape)\n",
    "print(\"Shape of y train:\", y_train.shape)\n",
    "print(\"Shape of X val:\", X_val.shape)\n",
    "print(\"Shape of y val:\", y_val.shape)\n",
    "print(\"Shape of X test:\", X_test.shape)\n",
    "print(\"Shape of y test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69aaf137-7976-43e9-aa89-e52f381f35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train after imputation:\n",
      "       no_wrk_aux  no2_wrk_aux  o3_wrk_aux       temp         rh\n",
      "0       0.028002     0.018061    0.032134   3.294843  52.182907\n",
      "1       0.030186     0.015001    0.029275  11.455113  65.229446\n",
      "2       0.028858     0.014326    0.029976  19.665237  35.056055\n",
      "3       0.045463     0.000057    0.017437  33.924103  31.951930\n",
      "4       0.029843     0.016397    0.035596  25.440354  64.711433\n",
      "...          ...          ...         ...        ...        ...\n",
      "5251    0.028483     0.018441    0.029313  19.278969  73.094890\n",
      "5252    0.028474     0.017283    0.033776   1.152403  35.993397\n",
      "5253    0.032067     0.013508    0.035968  31.530285  22.603555\n",
      "5254    0.034559     0.011188    0.033403  18.325056  39.961012\n",
      "5255    0.037868     0.006970    0.032114  29.046651  35.974374\n",
      "\n",
      "[5256 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dealing with missing data - SUB OUT WHEN WE LEARN MORE ADVANCED METHOD!\n",
    "# If you drop rows with data, should that happen before the split??? otherwise things will be different sizes\n",
    "# IF I WERE TO DO THIS, SHOULD DO THAT BEFORE THE SPLIT\n",
    "\n",
    "# can i just get rid of these index columns?\n",
    "# you only impute the input features, right?\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform the training data (impute missing values)\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "\n",
    "# Transform the validation and test sets using the same imputer\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Optionally, convert back to DataFrame for easier handling\n",
    "X_train_imputed_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "X_val_imputed_df = pd.DataFrame(X_val_imputed, columns=X_val.columns)\n",
    "X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "\n",
    "# reduce the columns again\n",
    "X_train_imputed_df = X_train_imputed_df[['no_wrk_aux','no2_wrk_aux','o3_wrk_aux','temp','rh']]\n",
    "X_val_imputed_df = X_val_imputed_df[['no_wrk_aux','no2_wrk_aux','o3_wrk_aux','temp','rh']]\n",
    "X_test_imputed_df = X_test_imputed_df[['no_wrk_aux','no2_wrk_aux','o3_wrk_aux','temp','rh']]\n",
    "\n",
    "# Output the results\n",
    "print(\"X_train after imputation:\\n\", X_train_imputed_df)\n",
    "#print(\"X_val after imputation:\\n\", X_val_imputed_df)\n",
    "#print(\"X_test after imputation:\\n\", X_test_imputed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c3a9678-c9ff-44a6-884d-fcb6c0ff61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic feature engineering - DOES THIS HAPPEN BEFORE OR AFTER SCALING AND DEALING WITH MISSING DATA?\n",
    "# YES, to get same mean and sd\n",
    "# doesn't work, so need to deal with missing data first\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(5, include_bias=False, interaction_only=True) # this last argument means that the one column is gone\n",
    "\n",
    "X_train_imputed_engn = poly.fit_transform(X_train_imputed_df) # [a, b, a^2, ab, b^2]\n",
    "X_train_imputed_engn_df = pd.DataFrame(X_train_imputed_engn) # HOW DO YOU GET THE COLUMN NAMES BACK FOR THIS?\n",
    "\n",
    "X_val_imputed_engn = poly.fit_transform(X_val_imputed_df) # [a, b, a^2, ab, b^2]\n",
    "X_val_imputed_engn_df = pd.DataFrame(X_val_imputed_engn) # HOW DO YOU GET THE COLUMN NAMES BACK FOR THIS?\n",
    "\n",
    "X_test_imputed_engn = poly.fit_transform(X_test_imputed_df) # [a, b, a^2, ab, b^2]\n",
    "X_test_imputed_engn_df = pd.DataFrame(X_test_imputed_engn) # HOW DO YOU GET THE COLUMN NAMES BACK FOR THIS?\n",
    "\n",
    "#print(X_val_imputed_engn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd9ed70b-61de-48b9-8ff3-eaf2d49adeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5256, 31)\n",
      "(1752, 31)\n",
      "(1752, 31)\n"
     ]
    }
   ],
   "source": [
    "# Scaling data using standard scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_imputed_engn_scaled = scaler.fit_transform(X_train_imputed_engn_df)\n",
    "X_val_imputed_engn_scaled = scaler.transform(X_val_imputed_engn_df)\n",
    "X_test_imputed_engn_scaled = scaler.transform(X_test_imputed_engn_df)\n",
    "\n",
    "# convert back to DataFrame \n",
    "X_train_imputed_engn_scaled_df = pd.DataFrame(X_train_imputed_engn_scaled, columns=X_train_imputed_engn_df.columns)\n",
    "X_val_imputed_engn_scaled_df = pd.DataFrame(X_val_imputed_engn_scaled, columns=X_val_imputed_engn_df.columns)\n",
    "X_test_imputed_engn_scaled_df = pd.DataFrame(X_test_imputed_engn_scaled, columns=X_test_imputed_engn_df.columns)\n",
    "\n",
    "\n",
    "print(X_train_imputed_engn_scaled_df.shape)\n",
    "print(X_val_imputed_engn_scaled_df.shape)\n",
    "print(X_test_imputed_engn_scaled_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
